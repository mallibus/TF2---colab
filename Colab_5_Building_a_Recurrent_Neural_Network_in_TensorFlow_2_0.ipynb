{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Colab 5 - Building a Recurrent Neural Network in TensorFlow 2.0.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mallibus/TF2---colab/blob/master/Colab_5_Building_a_Recurrent_Neural_Network_in_TensorFlow_2_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z82c-Fcay0a3",
        "colab_type": "text"
      },
      "source": [
        "## Step 1: Installing the dependencies and setting up a GPU environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMndQSr4O_nd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "outputId": "78507579-7d8c-4890-c036-d35b298e0f64"
      },
      "source": [
        "!pip install tensorflow-gpu==2.0.0.alpha0"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu==2.0.0.alpha0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/66/32cffad095253219d53f6b6c2a436637bbe45ac4e7be0244557210dc3918/tensorflow_gpu-2.0.0a0-cp36-cp36m-manylinux1_x86_64.whl (332.1MB)\n",
            "\u001b[K     |████████████████████████████████| 332.1MB 58kB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0.alpha0) (3.7.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0.alpha0) (1.12.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0.alpha0) (1.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0.alpha0) (1.15.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0.alpha0) (0.1.7)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0.alpha0) (0.2.2)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0.alpha0) (0.8.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0.alpha0) (1.16.4)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0.alpha0) (1.0.8)\n",
            "Collecting tb-nightly<1.14.0a20190302,>=1.14.0a20190301 (from tensorflow-gpu==2.0.0.alpha0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a9/51/aa1d756644bf4624c03844115e4ac4058eff77acd786b26315f051a4b195/tb_nightly-1.14.0a20190301-py3-none-any.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 44.7MB/s \n",
            "\u001b[?25hCollecting tf-estimator-nightly<1.14.0.dev2019030116,>=1.14.0.dev2019030115 (from tensorflow-gpu==2.0.0.alpha0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/82/f16063b4eed210dc2ab057930ac1da4fbe1e91b7b051a6c8370b401e6ae7/tf_estimator_nightly-1.14.0.dev2019030115-py2.py3-none-any.whl (411kB)\n",
            "\u001b[K     |████████████████████████████████| 419kB 50.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0.alpha0) (0.33.4)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0.alpha0) (1.1.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0.alpha0) (0.7.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==2.0.0.alpha0) (41.0.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==2.0.0.alpha0) (2.8.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow-gpu==2.0.0.alpha0) (3.1.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow-gpu==2.0.0.alpha0) (0.15.4)\n",
            "Installing collected packages: tb-nightly, tf-estimator-nightly, tensorflow-gpu\n",
            "Successfully installed tb-nightly-1.14.0a20190301 tensorflow-gpu-2.0.0a0 tf-estimator-nightly-1.14.0.dev2019030115\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorboard",
                  "tensorflow",
                  "tensorflow_estimator"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JL3SBH6PzDwV",
        "colab_type": "text"
      },
      "source": [
        "## Step 2: Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynShOu8nNtFt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import imdb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kw7-sPdOzf5l",
        "colab_type": "code",
        "outputId": "fae58c7b-ee4d-4977-d4dc-bfedb3124f6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.0.0-alpha0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEjlM2EazOf0",
        "colab_type": "text"
      },
      "source": [
        "## Step 3: Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wB0tNtXJzTfA",
        "colab_type": "text"
      },
      "source": [
        "### Setting up the dataset parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jw6_KU24SrYK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "number_of_words = 20000\n",
        "max_len = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePywR8A4zaxT",
        "colab_type": "text"
      },
      "source": [
        "### Loading the IMDB dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_giaPK-A9s3",
        "colab_type": "text"
      },
      "source": [
        "Cell added to allow imdb load - otherwise I got error  \n",
        "`ValueError: Object arrays cannot be loaded when allow_pickle=False`\n",
        "\n",
        "The fix comes from [here](https://stackoverflow.com/questions/55890813/how-to-fix-object-arrays-cannot-be-loaded-when-allow-pickle-false-for-imdb-loa)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIZzX9XT_mIV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "d3c17d30-5960-4fbc-d180-0ecf3e4bdd59"
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "old = np.load\n",
        "np.load = lambda *a,**k: old(*a,**k,allow_pickle=True)\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=number_of_words)\n",
        "\n",
        "np.load = old\n",
        "del(old)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kCTV_hjOKmE",
        "colab_type": "code",
        "outputId": "796755f2-d9e5-46bd-fee2-ae0f9417762f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        }
      },
      "source": [
        "# This does not work\n",
        "# (X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=number_of_words)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-e465d8d5fcfa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnumber_of_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/datasets/imdb.py\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(path, num_words, skip_top, maxlen, seed, start_char, oov_char, index_from, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m       file_hash='599dadb1135973df5b59232a0e9a887c')\n\u001b[1;32m     85\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x_train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y_train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m     \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x_test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y_test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    260\u001b[0m                 return format.read_array(bytes,\n\u001b[1;32m    261\u001b[0m                                          \u001b[0mallow_pickle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallow_pickle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m                                          pickle_kwargs=self.pickle_kwargs)\n\u001b[0m\u001b[1;32m    263\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/lib/format.py\u001b[0m in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[1;32m    694\u001b[0m         \u001b[0;31m# The array contained Python objects. We need to unpickle the data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 696\u001b[0;31m             raise ValueError(\"Object arrays cannot be loaded when \"\n\u001b[0m\u001b[1;32m    697\u001b[0m                              \"allow_pickle=False\")\n\u001b[1;32m    698\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpickle_kwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Object arrays cannot be loaded when allow_pickle=False"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZI4hMdYkGtx-",
        "colab_type": "text"
      },
      "source": [
        "## Explore some reviews\n",
        "\n",
        "To convert back from list of numbers to words with some sense, there is some work to do. [Here is where the code comes from](https://stackoverflow.com/questions/42821330/restore-original-text-from-keras-s-imdb-dataset)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSW_5FX1DRh6",
        "colab_type": "code",
        "outputId": "5386ea24-01f2-45c4-91fe-85d2bdf77018",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "NUM_WORDS=number_of_words # only use top  words\n",
        "INDEX_FROM=3   # word index offset\n",
        "\n",
        "word_to_id = imdb.get_word_index()\n",
        "word_to_id = {k:(v+INDEX_FROM) for k,v in word_to_id.items()}\n",
        "word_to_id[\"<PAD>\"] = 0\n",
        "word_to_id[\"<START>\"] = 1\n",
        "word_to_id[\"<UNK>\"] = 2\n",
        "\n",
        "id_to_word = {value:key for key,value in word_to_id.items()}\n",
        "#print(' '.join(id_to_word[id] for id in X_train[0] ))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "1646592/1641221 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ojs9DxxgCWZt",
        "colab_type": "code",
        "outputId": "9af4e535-ce5f-496b-fdaf-9bb4828c876e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        }
      },
      "source": [
        "import textwrap\n",
        "ylabels = ['Negative','Positive']\n",
        "size = 3\n",
        "sample = np.random.choice(range(len(X_train)),size)\n",
        "for i in sample:\n",
        "    print(f'Review #{i}:{ylabels[y_train[i]]}')\n",
        "    s = ' '.join(id_to_word[id] for id in X_train[i])\n",
        "    print('\\n'.join(textwrap.wrap(s, width=120, replace_whitespace=False)))\n",
        "    print()\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Review #5326:Positive\n",
            "<START> i realize this review will get me bashed by the expert film critics <UNK> this site but i will defend this film\n",
            "br br the dentist is actually a really good film the acting isn't always top notch but the thrills are good and the\n",
            "story's good plus you see linda hoffman's boobies not that i'm an expert in this field but the direction seems good and\n",
            "the plot makes sense corbin makes a great creepy dentist it does to dentists what jaws does to sharks ish it obviously\n",
            "had a fairly limited budget but they did well with it what they could and developed the characters well those that count\n",
            "br br the end\n",
            "\n",
            "Review #100:Negative\n",
            "<START> i am a great fan of david lynch and have everything that he's made on dvd except for hotel room the 2 hour twin\n",
            "peaks movie so when i found out about this i immediately grabbed it and and what is this it's a bunch of crudely drawn\n",
            "black and white cartoons that are loud and foul mouthed and unfunny maybe i don't know what's good but maybe this is\n",
            "just a bunch of crap that was <UNK> on the public under the name of david lynch to make a few bucks too let me make it\n",
            "clear that i didn't care about the foul language part but had to keep <UNK> the sound because my neighbors might have\n",
            "all in all this is a highly disappointing release and may well have just been left in the <UNK> box set as a curiosity i\n",
            "highly recommend you don't spend your money on this 2 out of 10\n",
            "\n",
            "Review #24073:Negative\n",
            "<START> set in <UNK> rhode island feeding the masses tries to be a satiric look at the role of the media in government\n",
            "at best it could be applied to how the us try to control media during the iraq war but it ends up feeling hollow there's\n",
            "never any really tension in the story and the acting never very good worst the direction of the movie is atrocious focus\n",
            "more on odd camera angles that fail to convey anything beyond isn't this an odd way to hold the camera special effects\n",
            "are pretty bad at one point video of an explosion is green screened over the city and it's laughable at best br br the\n",
            "film does have a couple bright spots namely the advertisements for post zombie services including a <UNK> service and a\n",
            "party bus but it's far too little to make the film worthwhile br br for a better zombie film try hide and creep it has\n",
            "the same weak production value but there's much more wit humor and talent behind it\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZKDNoTKzi5w",
        "colab_type": "text"
      },
      "source": [
        "### Padding all sequences to be the same length "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHcMNzv7Pd1s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Changing variable name to preserve orginal dataset\n",
        "X_train_p = tf.keras.preprocessing.sequence.pad_sequences(X_train, maxlen=max_len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fcxd--ESP3Rh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Changing variable name to preserve orginal dataset\n",
        "X_test_p = tf.keras.preprocessing.sequence.pad_sequences(X_test, maxlen=max_len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VG6LBKGnz7jT",
        "colab_type": "text"
      },
      "source": [
        "## Step 4: Building a Recurrent Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUVnz-9K0DcW",
        "colab_type": "text"
      },
      "source": [
        "### Defining the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2GHzwk6OMrV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.Sequential()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnXJZYR-0HXE",
        "colab_type": "text"
      },
      "source": [
        "### Adding the embedding layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWqC0DXbO9FU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.add(tf.keras.layers.Embedding(input_dim=number_of_words, output_dim=128, input_shape=(X_train_p.shape[1],)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWT_rMlaS8Kr",
        "colab_type": "code",
        "outputId": "dac2bf5b-9c41-47f6-ad7c-e905b2fb4bbf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 100, 128)          2560000   \n",
            "=================================================================\n",
            "Total params: 2,560,000\n",
            "Trainable params: 2,560,000\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CM-lpTZX1mEG",
        "colab_type": "text"
      },
      "source": [
        "### Adding the LSTM layer\n",
        "\n",
        "- units: 128\n",
        "- activation: tanh"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5W7IXqhjQpAl",
        "colab_type": "code",
        "outputId": "a50ecfb7-8869-41f8-f8cb-b73e61075cdf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "model.add(tf.keras.layers.LSTM(units=128, activation='tanh'))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0714 16:25:02.393356 140060086777728 tf_logging.py:161] <tensorflow.python.keras.layers.recurrent.UnifiedLSTM object at 0x7f61cc82cfd0>: Note that this layer is not optimized for performance. Please use tf.keras.layers.CuDNNLSTM for better performance on GPU.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9T9M5Ult10XM",
        "colab_type": "text"
      },
      "source": [
        "### Adding the output layer\n",
        "\n",
        "- units: 1\n",
        "- activation: sigmoid"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xe1nHzq7Q91-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWcqM4Yr2ALS",
        "colab_type": "text"
      },
      "source": [
        "### Compiling the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-z9ACOXcRUUN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PiolKKO6RjVF",
        "colab_type": "code",
        "outputId": "5269a227-579d-4441-b05a-6eb703a48b41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 100, 128)          2560000   \n",
            "_________________________________________________________________\n",
            "unified_lstm (UnifiedLSTM)   (None, 128)               131584    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 2,691,713\n",
            "Trainable params: 2,691,713\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bPUvbfe2GJI",
        "colab_type": "text"
      },
      "source": [
        "### Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FqUTA1CRpQ8",
        "colab_type": "code",
        "outputId": "3ae0fdfe-ef18-458a-9baf-aae4575d7bac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        }
      },
      "source": [
        "model.fit(X_train_p, y_train, epochs=3, batch_size=128)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "25000/25000 [==============================] - 6s 247us/sample - loss: 0.4627 - accuracy: 0.7846\n",
            "Epoch 2/3\n",
            "25000/25000 [==============================] - 2s 84us/sample - loss: 0.2910 - accuracy: 0.8840\n",
            "Epoch 3/3\n",
            "25000/25000 [==============================] - 2s 81us/sample - loss: 0.2304 - accuracy: 0.9111\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f61d0cf6fd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wMo2wYpbCgb",
        "colab_type": "text"
      },
      "source": [
        "### Evaluating the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8kD_6q-RySO",
        "colab_type": "code",
        "outputId": "b40c2f28-fb6b-41e1-aeab-7e0e06cddf2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "test_loss, test_acurracy = model.evaluate(X_test_p, y_test)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25000/25000 [==============================] - 3s 129us/sample - loss: 0.3421 - accuracy: 0.8535\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0XnUtS-cEeI",
        "colab_type": "code",
        "outputId": "eed2cbc4-81f5-48c5-e2f6-347c73786212",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "print(\"Test accuracy: {}\".format(test_acurracy))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy: 0.8534799814224243\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2lmZYwrZ465",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byyBa3ohZ5Ui",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model = tf.keras.Sequential()\n",
        "#model.add(tf.keras.layers.Embedding(input_dim=number_of_words, output_dim=128, input_shape=(X_train_p.shape[1],)))\n",
        "#model.add(tf.keras.layers.LSTM(units=128, activation='tanh'))\n",
        "#model.add(tf.keras.layers.GlobalAveragePooling1D())\n",
        "#model.add(tf.keras.layers.Dense(units=128, activation='relu'))\n",
        "#model.add(tf.keras.layers.BatchNormalization())\n",
        "#model.add(tf.keras.layers.Dropout(0.2))\n",
        "#model.add(tf.keras.layers.Dense(units=128, activation='relu'))\n",
        "#model.add(tf.keras.layers.BatchNormalization())\n",
        "#model.add(tf.keras.layers.Dropout(0.2))\n",
        "#model.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n",
        "#model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "#model.fit(X_train_p, y_train, epochs=3, batch_size=128)\n",
        "#test_loss, test_acurracy = model.evaluate(X_test_p, y_test)\n",
        "#print(\"Test accuracy: {}\".format(test_acurracy))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5c03_5-gToO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCvgbLnfZ51x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 901
        },
        "outputId": "afc8c5f6-c038-416c-c91c-ad9af7673718"
      },
      "source": [
        "ylabels = ['Negative','Positive']\n",
        "size = 3\n",
        "sample = np.random.choice(range(len(X_test)),size)\n",
        "predictions = model.predict_classes(X_test_p[sample])\n",
        "for idx,i in enumerate(sample):\n",
        "    print(f'Review #{i} - P:{ylabels[predictions[idx][0]]},A:{ylabels[y_test[sample[idx]]]}')\n",
        "    s = ' '.join(id_to_word[id] for id in X_test[i])\n",
        "    print('\\n'.join(textwrap.wrap(s, width=120, replace_whitespace=False)))\n",
        "    print()\n"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Review #12853 - P:Negative,A:Negative\n",
            "<START> i have just caught this movie on tcm and can understand why george murphy went into politics if this was the\n",
            "best mgm could serve up to him it is so slow moving that the attempt to make it a real film noir effort does not come\n",
            "off it featured two of my favourite br br players in eve arden completely wasted and dean stockwell the best actor in\n",
            "the film but what really hit me was that the leading lady frances <UNK> went through some 90 minutes it seemed longer\n",
            "without changing the expression on her face her <UNK> scene was comical john <UNK> played his role ok but the script let\n",
            "him and the rest of the cast down very badly i gave it 4 stars mainly because of the photography it would have been on\n",
            "the first half of the program when double features were the go\n",
            "\n",
            "Review #8656 - P:Positive,A:Positive\n",
            "<START> if you're in the middle of a ferocious war and it's still not clear that you're going to come out on top among\n",
            "the things you'll be concerned with is to keep up the morale of the civilians to demonstrate that our troops have the\n",
            "bravery the <UNK> and the dedication to overcome all the odds in a noble cause and that's just what director anthony\n",
            "<UNK> provided the british with naval war film we dive at dawn after more than 60 years it's not surprising that some of\n",
            "the movie is dated it doesn't help that the class stereotypes which help define the enlisted men from the officers can\n",
            "be jarring here as in so many other british war films the men invariably have thick regional working class accents while\n",
            "the officers speak with an educated that would place them at home in england's finest ruling class <UNK> in this movie\n",
            "freddie taylor john mills the captain of the submarine sea tiger is clever confident resourceful aggressive in control\n",
            "good with his men humorous with his peers quick to make a decision and it helps that he's lucky his men are jolly for\n",
            "the most part competent at their jobs and always ready with a joke when things get tense although we spend the first\n",
            "third of the movie getting to know these people while they're on leave after that things get tense quickly br br taylor\n",
            "and his sub are ordered to destroy the a new german <UNK> they just miss the ship when it enters the <UNK> canal and\n",
            "heads into the <UNK> taylor the risks and decides the sea tiger will go after it through mine fields anti sub <UNK> and\n",
            "with a real risk of not having enough fuel to return to home base after several tense situations the confrontation takes\n",
            "place the sea tiger lets loose six <UNK> but has to dive not knowing if it had done its job after a clever <UNK> taylor\n",
            "a couple of german <UNK> but then realizes there is not enough fuel he plans to <UNK> his sub and surrender when just at\n",
            "the last moment james hobson eric portman a seaman who had been sullen and a loner and who speaks german says there is a\n",
            "small danish coastal village that had been a fuel <UNK> he thinks it might still be for the germans the last third of\n",
            "the movie is a rousing action sequence as the crew of the sub attempts to hold off the germans long enough to pump in\n",
            "enough fuel to get the sea tiger back to britain this is a wartime propaganda movie so don't expect failure and did the\n",
            "sea tiger actually put the down are the men reunited with their wives and sweethearts did hobson have a reconciliation\n",
            "with his wife and small son that left him smiling for once did freddie taylor finally have a chance to make use of all\n",
            "those female names in his little black book you'll have to see the movie br br there are propaganda war movies and there\n",
            "are propaganda war movies some like powell's and <UNK> one of our aircraft is missing and the <UNK> parallel still stand\n",
            "up to viewing today because the stories are solid and unexpected and the creators didn't use obvious <UNK> clichés\n",
            "others like we dive at dawn were made with enough clichés that when watching we have to remind ourselves how dire the\n",
            "time was when the film was made still <UNK> can build a lot of suspense even with a few clichés the sea <UNK> forcing\n",
            "its way through a sub net was tense the stalking of the and the plotting needed for the <UNK> firing was realistic john\n",
            "<UNK> no nonsense attitude while he prepared to attack was well handled the fake out preparations to make the sea tiger\n",
            "look as if it had been destroyed by depth charges was as realistic inside the sub as well as out as you could hope for\n",
            "and the battle for the fuel <UNK> was dramatic and exciting we dive at dawn is not a classic war film but it's a well\n",
            "made well acted example of its type and time br br john mills it's worth noting had a long long career especially in the\n",
            "fifties he played in a number of serious minded films looking back at those wwii days he had the quality of showing grit\n",
            "<UNK> and perseverance but of also being <UNK> a man england could be proud of as he fought the war top billed in this\n",
            "movie was eric portman a fine actor with a unique voice and the ability to give stares so cold you'd want to put on a\n",
            "sweater everyone on the sub is very much in the joking but stiff upper lip mode but portman manages some complexity for\n",
            "his character mills and portman did fine jobs working together on this film\n",
            "\n",
            "Review #16003 - P:Positive,A:Positive\n",
            "<START> i really enjoyed the patriot this movie had less violence and was based on a real life threat that could\n",
            "inevitably destroy our civilization one line in the movie from wesley <UNK> seagal stuck out in my mind to be very true\n",
            "of our society western medicine is in the practice of <UNK> illness and i am in the business of <UNK> it br br\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1L6KkEDcg7sU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}